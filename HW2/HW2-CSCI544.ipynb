{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CSCI544 HW2\n",
    "Yuhang Xiao - 6913860906 - yxiao776@usc.edu"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Environment requirements are listed in requirements.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Report\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Dataset Generation\n",
    "Using the same data source as HW1 and the same cleaning and preprocessing procedures. \n",
    "#### Word Embedding\n",
    "Two examples of semantic similarity:\n",
    "- China - Beijing + Tokyo = Japan\n",
    "- big ~ huge\n",
    "\n",
    "(a) Pretrained Word2Vec:\n",
    "- China - Beijing + Tokyo = [('Japan', 0.8236700892448425)]\n",
    "- big ~ huge: 0.7809856\n",
    "\n",
    "(b) Trained Word2Vec:\n",
    "- China - Beijing + Tokyo = [('Japan', 0.6667723655700684)]\n",
    "- big ~ huge: 0.7743653\n",
    "\n",
    "We can conclude that both models could encode the words close to their semantic similar words in the word embedding space. \n",
    "Our own trained Word2Vec could extract the same similar word as the pretrained model does, even though it's only trained on the review corpus.\n",
    "However, it seems that the pretrained model is still better because it achieves higher scores in similarity. \n",
    "#### Simple Models\n",
    "Binary Classfication Accuracy\n",
    "\n",
    "Perceptron:\n",
    "- Pretrained word2vec-google-news-300: 0.853225\n",
    "- Trained Word2Vec: 0.8785\n",
    "- TF-IDF: 0.930925\n",
    "\n",
    "SVM:\n",
    "- Pretrained word2vec-google-news-300: 0.8719\n",
    "- Trained Word2Vec: 0.910625\n",
    "- TF-IDF: 0.938175\n",
    "\n",
    "We can conclude that using TF-IDF features gets the best performance, which may be because that we only keep the important words that matter for sentimental analysis and TF-IDF is better at capturing information of important words. Besides, a simple average pooling of each review for Word2Vec may harm its performance. Our own trained Word2Vec has higher accuracies than pretrained Word2Vec, which may be because our own Word2Vec is trained on the reviews directly that captures features better in the context of reviews.\n",
    "#### Feedforward Neural Networks\n",
    "Implementation: Flattened feature vectors go through two hidden layers with 50 and 10 nodes respectively, then a predicition head with 2 (for binary class) or 3 (for ternary class) is attached to predict the class label. Cross entropy loss is used and the FNN is optimized by AdamW optimizer with a cosine annealing learning rate scheduler.\n",
    "\n",
    "(a) Average Pooling Feature\n",
    "\n",
    "Binary classification accuracy:\n",
    "- Pretrained Word2Vec: 0.90195\n",
    "- Trained Word2Vec: 0.92155\n",
    "\n",
    "Ternary classification accuracy:\n",
    "- Pretrained Word2Vec: 0.7614\n",
    "- Trained Word2Vec: 0.79716\n",
    "\n",
    "(b) 10-word Sequence Feature\n",
    "\n",
    "Binary classification accuracy:\n",
    "- Pretrained Word2Vec: 0.85345\n",
    "- Trained Word2Vec: 0.8655\n",
    "\n",
    "Ternary classification accuracy:\n",
    "- Pretrained Word2Vec: 0.70042\n",
    "- Trained Word2Vec: 0.7196\n",
    "\n",
    "By using FFN models, word2vec features can get better results compared to using them on simple models, such as Perceptron and SVM. When using the same average pooling strategy, word2vec features on FFN models can get very close preformances compared to TF-IDF features on simple models. When using the first 10 words concatenation strategy, the performances are worse, which may be because the features don't capture the information of the whole review. Besides, our own trained word2vec gets better results than pretrained word2vec as in simple models.\n",
    "#### Convolutional Neural Networks\n",
    "Implementation: Feature vectors with a sequence length of 50 first go through two CNN layers with 50 and 10 output channels respectively, which are both using kernels with kernel size = 3, padding = 1, stride = 1 so that the sequence length is preserved. Then a maxpooling layer is applied to squeeze the sequence length. Finally, a predicition head with 2 (for binary class) or 3 (for ternary class) is attached to predict the class label. Cross entropy loss is used and the CNN is optimized by AdamW optimizer with a cosine annealing learning rate scheduler.\n",
    "\n",
    "Binary classification accuracy:\n",
    "- Pretrained Word2Vec: 0.919525\n",
    "- Trained Word2Vec: 0.9204\n",
    "\n",
    "Ternary classification accuracy:\n",
    "- Pretrained Word2Vec: 0.77694\n",
    "- Trained Word2Vec: 0.78948"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 284,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to /lab/mydcxiao/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /lab/mydcxiao/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to /lab/mydcxiao/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import nltk\n",
    "nltk.download('wordnet')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('punkt')\n",
    "import re\n",
    "from bs4 import BeautifulSoup\n",
    "import contractions\n",
    "import warnings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rows:  2640254\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>marketplace</th>\n",
       "      <th>customer_id</th>\n",
       "      <th>review_id</th>\n",
       "      <th>product_id</th>\n",
       "      <th>product_parent</th>\n",
       "      <th>product_title</th>\n",
       "      <th>product_category</th>\n",
       "      <th>star_rating</th>\n",
       "      <th>helpful_votes</th>\n",
       "      <th>total_votes</th>\n",
       "      <th>vine</th>\n",
       "      <th>verified_purchase</th>\n",
       "      <th>review_headline</th>\n",
       "      <th>review_body</th>\n",
       "      <th>review_date</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>US</td>\n",
       "      <td>43081963</td>\n",
       "      <td>R18RVCKGH1SSI9</td>\n",
       "      <td>B001BM2MAC</td>\n",
       "      <td>307809868</td>\n",
       "      <td>Scotch Cushion Wrap 7961, 12 Inches x 100 Feet</td>\n",
       "      <td>Office Products</td>\n",
       "      <td>5</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>N</td>\n",
       "      <td>Y</td>\n",
       "      <td>Five Stars</td>\n",
       "      <td>Great product.</td>\n",
       "      <td>2015-08-31</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>US</td>\n",
       "      <td>10951564</td>\n",
       "      <td>R3L4L6LW1PUOFY</td>\n",
       "      <td>B00DZYEXPQ</td>\n",
       "      <td>75004341</td>\n",
       "      <td>Dust-Off Compressed Gas Duster, Pack of 4</td>\n",
       "      <td>Office Products</td>\n",
       "      <td>5</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>N</td>\n",
       "      <td>Y</td>\n",
       "      <td>Phffffffft, Phfffffft. Lots of air, and it's C...</td>\n",
       "      <td>What's to say about this commodity item except...</td>\n",
       "      <td>2015-08-31</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>US</td>\n",
       "      <td>21143145</td>\n",
       "      <td>R2J8AWXWTDX2TF</td>\n",
       "      <td>B00RTMUHDW</td>\n",
       "      <td>529689027</td>\n",
       "      <td>Amram Tagger Standard Tag Attaching Tagging Gu...</td>\n",
       "      <td>Office Products</td>\n",
       "      <td>5</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>N</td>\n",
       "      <td>Y</td>\n",
       "      <td>but I am sure I will like it.</td>\n",
       "      <td>Haven't used yet, but I am sure I will like it.</td>\n",
       "      <td>2015-08-31</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>US</td>\n",
       "      <td>52782374</td>\n",
       "      <td>R1PR37BR7G3M6A</td>\n",
       "      <td>B00D7H8XB6</td>\n",
       "      <td>868449945</td>\n",
       "      <td>AmazonBasics 12-Sheet High-Security Micro-Cut ...</td>\n",
       "      <td>Office Products</td>\n",
       "      <td>1</td>\n",
       "      <td>2.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>N</td>\n",
       "      <td>Y</td>\n",
       "      <td>and the shredder was dirty and the bin was par...</td>\n",
       "      <td>Although this was labeled as &amp;#34;new&amp;#34; the...</td>\n",
       "      <td>2015-08-31</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>US</td>\n",
       "      <td>24045652</td>\n",
       "      <td>R3BDDDZMZBZDPU</td>\n",
       "      <td>B001XCWP34</td>\n",
       "      <td>33521401</td>\n",
       "      <td>Derwent Colored Pencils, Inktense Ink Pencils,...</td>\n",
       "      <td>Office Products</td>\n",
       "      <td>4</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>N</td>\n",
       "      <td>Y</td>\n",
       "      <td>Four Stars</td>\n",
       "      <td>Gorgeous colors and easy to use</td>\n",
       "      <td>2015-08-31</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  marketplace  customer_id       review_id  product_id  product_parent  \\\n",
       "0          US     43081963  R18RVCKGH1SSI9  B001BM2MAC       307809868   \n",
       "1          US     10951564  R3L4L6LW1PUOFY  B00DZYEXPQ        75004341   \n",
       "2          US     21143145  R2J8AWXWTDX2TF  B00RTMUHDW       529689027   \n",
       "3          US     52782374  R1PR37BR7G3M6A  B00D7H8XB6       868449945   \n",
       "4          US     24045652  R3BDDDZMZBZDPU  B001XCWP34        33521401   \n",
       "\n",
       "                                       product_title product_category  \\\n",
       "0     Scotch Cushion Wrap 7961, 12 Inches x 100 Feet  Office Products   \n",
       "1          Dust-Off Compressed Gas Duster, Pack of 4  Office Products   \n",
       "2  Amram Tagger Standard Tag Attaching Tagging Gu...  Office Products   \n",
       "3  AmazonBasics 12-Sheet High-Security Micro-Cut ...  Office Products   \n",
       "4  Derwent Colored Pencils, Inktense Ink Pencils,...  Office Products   \n",
       "\n",
       "  star_rating  helpful_votes  total_votes vine verified_purchase  \\\n",
       "0           5            0.0          0.0    N                 Y   \n",
       "1           5            0.0          1.0    N                 Y   \n",
       "2           5            0.0          0.0    N                 Y   \n",
       "3           1            2.0          3.0    N                 Y   \n",
       "4           4            0.0          0.0    N                 Y   \n",
       "\n",
       "                                     review_headline  \\\n",
       "0                                         Five Stars   \n",
       "1  Phffffffft, Phfffffft. Lots of air, and it's C...   \n",
       "2                      but I am sure I will like it.   \n",
       "3  and the shredder was dirty and the bin was par...   \n",
       "4                                         Four Stars   \n",
       "\n",
       "                                         review_body review_date  \n",
       "0                                     Great product.  2015-08-31  \n",
       "1  What's to say about this commodity item except...  2015-08-31  \n",
       "2    Haven't used yet, but I am sure I will like it.  2015-08-31  \n",
       "3  Although this was labeled as &#34;new&#34; the...  2015-08-31  \n",
       "4                    Gorgeous colors and easy to use  2015-08-31  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dtype={7: object}\n",
    "url = 'https://web.archive.org/web/20201127142707if_/https://s3.amazonaws.com/amazon-reviews-pds/tsv/amazon_reviews_us_Office_Products_v1_00.tsv.gz'\n",
    "data = pd.read_csv(url, sep='\\t', on_bad_lines='skip', dtype=dtype)\n",
    "# path = 'amazon_reviews_us_Office_Products_v1_00.tsv.gz'\n",
    "# data = pd.read_csv(path, sep='\\t', on_bad_lines='skip', dtype=dtype)\n",
    "# unzipped_path = 'amazon_reviews_us_Office_Products_v1_00.tsv'\n",
    "# data = pd.read_csv(unzipped_path, sep='\\t', on_bad_lines='skip', dtype=dtype)\n",
    "print(\"Rows: \", data.shape[0])\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Keep Reviews and Ratings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rows:  2640037\n",
      "Three sample reviews:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>star_rating</th>\n",
       "      <th>review_headline</th>\n",
       "      <th>review_body</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>246582</th>\n",
       "      <td>5</td>\n",
       "      <td>Fast Shipping!</td>\n",
       "      <td>Fast Shipping.  Works Perfectly!</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2639050</th>\n",
       "      <td>5</td>\n",
       "      <td>it works!</td>\n",
       "      <td>5 stars based on transmission clarity.  compar...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>697475</th>\n",
       "      <td>5</td>\n",
       "      <td>Five Stars</td>\n",
       "      <td>Love it, looks great on my wall!</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        star_rating review_headline  \\\n",
       "246582            5  Fast Shipping!   \n",
       "2639050           5       it works!   \n",
       "697475            5      Five Stars   \n",
       "\n",
       "                                               review_body  \n",
       "246582                    Fast Shipping.  Works Perfectly!  \n",
       "2639050  5 stars based on transmission clarity.  compar...  \n",
       "697475                    Love it, looks great on my wall!  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = data[['star_rating', 'review_headline', 'review_body']]\n",
    "data.dropna(inplace=True)\n",
    "print(\"Rows: \", data.shape[0])\n",
    "print(\"Three sample reviews:\")\n",
    "data.sample(3, random_state=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All reviews:  2640037\n",
      "1 rating reviews: 306962, 11.627185528081615%\n",
      "2 rating reviews: 138380, 5.241593204943719%\n",
      "3 rating reviews: 193674, 7.336033548014668%\n",
      "4 rating reviews: 418339, 15.845952159003833%\n",
      "5 rating reviews: 1582682, 59.949235559956165%\n"
     ]
    }
   ],
   "source": [
    "data['star_rating'] = pd.to_numeric(data['star_rating'], errors='coerce')\n",
    "print(\"All reviews: \", data.shape[0])\n",
    "print(f\"1 rating reviews: {data[data['star_rating'] == 1].shape[0]}, {100 * data[data['star_rating'] == 1].shape[0]/data.shape[0]}%\")\n",
    "print(f\"2 rating reviews: {data[data['star_rating'] == 2].shape[0]}, {100 * data[data['star_rating'] == 2].shape[0]/data.shape[0]}%\")\n",
    "print(f\"3 rating reviews: {data[data['star_rating'] == 3].shape[0]}, {100 * data[data['star_rating'] == 3].shape[0]/data.shape[0]}%\")\n",
    "print(f\"4 rating reviews: {data[data['star_rating'] == 4].shape[0]}, {100 * data[data['star_rating'] == 4].shape[0]/data.shape[0]}%\")\n",
    "print(f\"5 rating reviews: {data[data['star_rating'] == 5].shape[0]}, {100 * data[data['star_rating'] == 5].shape[0]/data.shape[0]}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ## We form three classes and select 50000 reviews randomly from each star ratings.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All reviews:  2640037\n",
      "Positive reviews:  2001021\n",
      "Negative reviews:  445342\n",
      "Neutral reviews:  193674\n",
      "All reviews == Positive + Negative + Neutral:  True\n"
     ]
    }
   ],
   "source": [
    "pos_reviews = data[data['star_rating'] > 3]\n",
    "neg_reviews = data[data['star_rating'] <= 2]\n",
    "neu_reviews = data[data['star_rating'] == 3]\n",
    "print(\"All reviews: \", data.shape[0])\n",
    "print(\"Positive reviews: \", pos_reviews.shape[0])\n",
    "print(\"Negative reviews: \", neg_reviews.shape[0])\n",
    "print(\"Neutral reviews: \", neu_reviews.shape[0])\n",
    "print(\"All reviews == Positive + Negative + Neutral: \", data.shape[0] == pos_reviews.shape[0] + neg_reviews.shape[0] + neu_reviews.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rows:  250000\n"
     ]
    }
   ],
   "source": [
    "pos_samples = pos_reviews.sample(n=100000, random_state=0)\n",
    "neg_samples = neg_reviews.sample(n=100000, random_state=0)\n",
    "neu_samples = neu_reviews.sample(n=50000, random_state=0)\n",
    "pos_samples['star_rating'] = 1\n",
    "neg_samples['star_rating'] = 2\n",
    "neu_samples['star_rating'] = 3\n",
    "pos_samples.rename(columns={'star_rating': 'label'}, inplace=True)\n",
    "neg_samples.rename(columns={'star_rating': 'label'}, inplace=True)\n",
    "neu_samples.rename(columns={'star_rating': 'label'}, inplace=True)\n",
    "dataset = pd.concat([pos_samples, neg_samples, neu_samples])\n",
    "dataset['review'] = dataset[['review_headline', 'review_body']].agg(' '.join, axis=1)\n",
    "dataset.drop(columns=['review_headline', 'review_body'], inplace=True)\n",
    "print(\"Rows: \", dataset.shape[0])\n",
    "# print(\"Three sample reviews:\")\n",
    "# dataset.sample(3, random_state=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Cleaning\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average length of reviews before cleaning:  353.398672\n",
      "Three sample reviews:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "      <th>review</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>508714</th>\n",
       "      <td>3</td>\n",
       "      <td>Good Phone but Major Design Flaw The Phones th...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>713271</th>\n",
       "      <td>1</td>\n",
       "      <td>Five Stars Great</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3017</th>\n",
       "      <td>3</td>\n",
       "      <td>Three Stars It's a little less sturdy than the...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        label                                             review\n",
       "508714      3  Good Phone but Major Design Flaw The Phones th...\n",
       "713271      1                                   Five Stars Great\n",
       "3017        3  Three Stars It's a little less sturdy than the..."
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(\"Average length of reviews before cleaning: \", dataset['review'].str.len().mean())\n",
    "print(\"Three sample reviews:\")\n",
    "dataset.sample(3,random_state=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average length of reviews after cleaning:  335.85226\n",
      "Three sample reviews:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "      <th>review</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>508714</th>\n",
       "      <td>3</td>\n",
       "      <td>good phone but major design flaw the phones th...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>713271</th>\n",
       "      <td>1</td>\n",
       "      <td>five stars great</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3017</th>\n",
       "      <td>3</td>\n",
       "      <td>three stars it s a little less sturdy than the...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        label                                             review\n",
       "508714      3  good phone but major design flaw the phones th...\n",
       "713271      1                                   five stars great\n",
       "3017        3  three stars it s a little less sturdy than the..."
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from bs4 import MarkupResemblesLocatorWarning\n",
    "warnings.filterwarnings(\"ignore\", category=MarkupResemblesLocatorWarning)\n",
    "dataset['review'] = dataset['review'].apply(str.lower)\n",
    "dataset['review'] = dataset['review'].apply(lambda x: re.sub(r'https?://\\S+|www\\.\\S+', '', x))\n",
    "dataset['review'] = dataset['review'].apply(lambda x: BeautifulSoup(x, \"html.parser\").text)\n",
    "dataset['review'] = dataset['review'].apply(lambda x: re.sub(r'[^a-zA-Z]+', ' ', x))\n",
    "dataset['review'] = dataset['review'].apply(lambda x: re.sub(r'\\s+', ' ', x).strip())\n",
    "dataset['review'] = dataset['review'].apply(lambda x: ' '.join([contractions.fix(word) for word in x.split()]))\n",
    "\n",
    "print(\"Average length of reviews after cleaning: \", dataset['review'].str.len().mean())\n",
    "print(\"Three sample reviews:\")\n",
    "dataset.sample(3,random_state=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pre-processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average length of reviews before preprocessing:  335.85226\n",
      "Three sample reviews:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "      <th>review</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>508714</th>\n",
       "      <td>3</td>\n",
       "      <td>good phone but major design flaw the phones th...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>713271</th>\n",
       "      <td>1</td>\n",
       "      <td>five stars great</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3017</th>\n",
       "      <td>3</td>\n",
       "      <td>three stars it s a little less sturdy than the...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        label                                             review\n",
       "508714      3  good phone but major design flaw the phones th...\n",
       "713271      1                                   five stars great\n",
       "3017        3  three stars it s a little less sturdy than the..."
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(\"Average length of reviews before preprocessing: \", dataset['review'].str.len().mean())\n",
    "print(\"Three sample reviews:\")\n",
    "dataset.sample(3,random_state=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## remove the stop words "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Three sample reviews:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "      <th>review</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>508714</th>\n",
       "      <td>3</td>\n",
       "      <td>good phone major design flaw phones work good ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>713271</th>\n",
       "      <td>1</td>\n",
       "      <td>five stars great</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3017</th>\n",
       "      <td>3</td>\n",
       "      <td>three stars little less sturdy previous versio...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        label                                             review\n",
       "508714      3  good phone major design flaw phones work good ...\n",
       "713271      1                                   five stars great\n",
       "3017        3  three stars little less sturdy previous versio..."
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "stopwords = set(stopwords.words('english'))\n",
    "dataset['review'] = dataset['review'].apply(lambda x: ' '.join([word for word in word_tokenize(x) if word not in stopwords]))\n",
    "print(\"Three sample reviews:\")\n",
    "dataset.sample(3,random_state=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## perform lemmatization  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average length of reviews after preprocessing:  209.386508\n",
      "Three sample reviews:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "      <th>review</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>508714</th>\n",
       "      <td>3</td>\n",
       "      <td>good phone major design flaw phone work good f...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>713271</th>\n",
       "      <td>1</td>\n",
       "      <td>five star great</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3017</th>\n",
       "      <td>3</td>\n",
       "      <td>three star little le sturdy previous version b...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        label                                             review\n",
       "508714      3  good phone major design flaw phone work good f...\n",
       "713271      1                                    five star great\n",
       "3017        3  three star little le sturdy previous version b..."
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.stem import WordNetLemmatizer\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "dataset['review'] = dataset['review'].apply(lambda x: ' '.join([lemmatizer.lemmatize(word) for word in word_tokenize(x)]))\n",
    "print(\"Average length of reviews after preprocessing: \", dataset['review'].str.len().mean())\n",
    "print(\"Three sample reviews:\")\n",
    "dataset.sample(3,random_state=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Word Embedding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (a) Pretrained Word2Vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[==================================================] 100.0% 1662.8/1662.8MB downloaded\n"
     ]
    }
   ],
   "source": [
    "import gensim.downloader as api\n",
    "wv = api.load('word2vec-google-news-300')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Two examples of semantic similarities \\\n",
    "(1) China - Beijing + Tokyo = Japan \\\n",
    "(2) big ~ huge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "China - Beijing + Tokyo = [('Japan', 0.8236700892448425)]\n",
      "big ~ huge: 0.7809856\n"
     ]
    }
   ],
   "source": [
    "# example 1\n",
    "print(\"China - Beijing + Tokyo =\", wv.most_similar(positive=['China', 'Tokyo'], negative=['Beijing'], topn=1))\n",
    "# example 2\n",
    "print(\"big ~ huge:\", wv.similarity('big', 'huge'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (b) Trained Word2Vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gensim.models\n",
    "corpus = data[['review_headline', 'review_body']]\n",
    "corpus['review'] = corpus[['review_headline', 'review_body']].agg(' '.join, axis=1)\n",
    "corpus.drop(columns=['review_headline', 'review_body'], inplace=True)\n",
    "corpus['review'] = corpus['review'].apply(str)\n",
    "model = gensim.models.Word2Vec(corpus['review'].apply(word_tokenize), vector_size=300, window=11, min_count=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Comparison of the two examples with pretrained model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "China - Beijing + Tokyo = [('Japan', 0.6667723655700684)]\n",
      "big ~ huge: 0.7743653\n"
     ]
    }
   ],
   "source": [
    "# example 1\n",
    "print(\"China - Beijing + Tokyo =\", model.wv.most_similar(positive=['China', 'Tokyo'], negative=['Beijing'], topn=1))\n",
    "# example 2\n",
    "print(\"big ~ huge:\", model.wv.similarity('big', 'huge'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can conclude that both models could encode the words close to their semantic similar words in the word embedding space. \n",
    "Our own trained Word2Vec could extract the same similar word as the pretrained model does, even though it's only trained on the review corpus.\n",
    "However, it seems that the pretrained model is still better because it achieves higher scores in similarity. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Simple Models\n",
    "\n",
    "Only use class 1 and class 2 for binary classification."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TF-IDF Feature Extraction "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(200000, 6740105)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "vectorizer = TfidfVectorizer(ngram_range=(1, 3))\n",
    "vectors = vectorizer.fit_transform(dataset['review'][dataset['label'] != 3])\n",
    "print(vectors.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train-Test Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set size:  (160000,) (160000, 6740105) (160000,)\n",
      "Test set size:  (40000,) (40000, 6740105) (40000,)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "labels = dataset['label'][dataset['label'] != 3]\n",
    "reviews = dataset['review'][dataset['label'] != 3]\n",
    "corpus_train, corpus_test, x_train, x_test, y_train, y_test = train_test_split(reviews, vectors, labels, test_size=0.2, random_state=42, stratify=labels)\n",
    "print(\"Training set size: \", corpus_train.shape, x_train.shape, y_train.shape)\n",
    "print(\"Test set size: \", corpus_test.shape, x_test.shape, y_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Average the Corpus Using Word2Vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(160000, 300) (160000, 300)\n",
      "(40000, 300) (40000, 300)\n"
     ]
    }
   ],
   "source": [
    "def parse_then_average(wv, sentence):\n",
    "    words = word_tokenize(sentence)\n",
    "    words = [word for word in words if word in wv.key_to_index]\n",
    "    if len(words) == 0:\n",
    "        return np.zeros(wv.vector_size)\n",
    "    return np.mean(wv[words], axis=0)\n",
    "\n",
    "pretrained_word_embeddings = np.vstack(corpus_train.apply(lambda x: parse_then_average(wv, x)))\n",
    "trained_word_embeddings = np.vstack(corpus_train.apply(lambda x: parse_then_average(model.wv, x)))\n",
    "pretrained_word_embeddings_test = np.vstack(corpus_test.apply(lambda x: parse_then_average(wv, x)))\n",
    "trained_word_embeddings_test = np.vstack(corpus_test.apply(lambda x: parse_then_average(model.wv, x)))\n",
    "print(pretrained_word_embeddings.shape, trained_word_embeddings.shape)\n",
    "print(pretrained_word_embeddings_test.shape, trained_word_embeddings_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Perceptron"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 275,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pretrained word2vec-google-news-300: 0.853225\n",
      "Trained Word2Vec: 0.8785\n",
      "TF-IDF: 0.930925\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.linear_model import Perceptron\n",
    "perceptron1 = Perceptron(tol=1e-2)\n",
    "perceptron2 = Perceptron()\n",
    "perceptron3 = Perceptron()\n",
    "perceptron1.fit(pretrained_word_embeddings, y_train)\n",
    "perceptron2.fit(trained_word_embeddings, y_train)\n",
    "perceptron3.fit(x_train, y_train)\n",
    "pred_test1 = perceptron1.predict(pretrained_word_embeddings_test)\n",
    "pred_test2 = perceptron2.predict(trained_word_embeddings_test)\n",
    "pred_test3 = perceptron3.predict(x_test)\n",
    "print(\"Pretrained word2vec-google-news-300:\", accuracy_score(y_test, pred_test1))\n",
    "print(\"Trained Word2Vec:\", accuracy_score(y_test, pred_test2))\n",
    "print(\"TF-IDF:\", accuracy_score(y_test, pred_test3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pretrained word2vec-google-news-300: 0.8719\n",
      "Trained Word2Vec: 0.910625\n",
      "TF-IDF: 0.938175\n"
     ]
    }
   ],
   "source": [
    "from sklearn.svm import LinearSVC\n",
    "svm1 = LinearSVC(dual='auto', random_state=0)\n",
    "svm2 = LinearSVC(dual='auto', random_state=0)\n",
    "svm3 = LinearSVC(dual='auto', random_state=0)\n",
    "svm1.fit(pretrained_word_embeddings, y_train)\n",
    "svm2.fit(trained_word_embeddings, y_train)\n",
    "svm3.fit(x_train, y_train)\n",
    "pred_test1 = svm1.predict(pretrained_word_embeddings_test)\n",
    "pred_test2 = svm2.predict(trained_word_embeddings_test)\n",
    "pred_test3 = svm3.predict(x_test)\n",
    "print(\"Pretrained word2vec-google-news-300:\", accuracy_score(y_test, pred_test1))\n",
    "print(\"Trained Word2Vec:\", accuracy_score(y_test, pred_test2))\n",
    "print(\"TF-IDF:\", accuracy_score(y_test, pred_test3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can conclude that using TF-IDF features gets the best performance, which may be because that we only keep the important words of the reviews and TF-IDF is better at capturing information of important words. Besides, a simple average pooling of each review for Word2Vec may harm its performance. Our own trained Word2Vec has higher accuracies than pretrained Word2Vec, which may be because our own Word2Vec is trained on the reviews directly that captures features better in the context of reviews."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feedforward Neural Networks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train-Test Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set size:  (200000,) (200000,)\n",
      "Test set size:  (50000,) (50000,)\n"
     ]
    }
   ],
   "source": [
    "labels = dataset['label']\n",
    "reviews = dataset['review']\n",
    "reviews_train, reviews_test, labels_train, labels_test = train_test_split(reviews, labels, test_size=0.2, random_state=42, stratify=labels)\n",
    "print(\"Training set size: \", reviews_train.shape, labels_train.shape)\n",
    "print(\"Test set size: \", reviews_test.shape, labels_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataset Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 223,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "class ReviewsDataset(Dataset):\n",
    "    def __init__(self, reviews, labels, wv, max_seq_len=10, average_pooling=True):\n",
    "        self.reviews = reviews\n",
    "        self.labels = labels\n",
    "        self.wv = wv\n",
    "        self.average_pooling = average_pooling\n",
    "        self.max_seq_len = max_seq_len\n",
    "    def __len__(self):\n",
    "        return self.reviews.shape[0]\n",
    "    def __getitem__(self, idx):\n",
    "        if self.average_pooling:\n",
    "            sent_embedding = torch.from_numpy(parse_then_average(self.wv, self.reviews.iloc[idx])).unsqueeze(0) # 1 x 300\n",
    "        else:\n",
    "            sentence = word_tokenize(self.reviews.iloc[idx])\n",
    "            word_embedding_list = []\n",
    "            i, j = 0, 0\n",
    "            while i < self.max_seq_len:\n",
    "                if j >= len(sentence):\n",
    "                    word_embedding_list.append(torch.zeros(self.wv.vector_size))\n",
    "                    i += 1\n",
    "                else:\n",
    "                    if sentence[j] in self.wv.key_to_index:\n",
    "                        word_embedding_list.append(torch.from_numpy(self.wv[sentence[j]].copy()))\n",
    "                        i += 1\n",
    "                        j += 1\n",
    "                    else:\n",
    "                        j += 1\n",
    "            sent_embedding = torch.stack(word_embedding_list) # 10 x 300\n",
    "        return sent_embedding.float(), self.labels.iloc[idx] - 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Engine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "def train_model(model, dataloader, criterion, optimizer, scheduler, num_epochs=10):\n",
    "    device = next(model.parameters()).device\n",
    "    for epoch in tqdm(range(num_epochs), desc=\"Training\"):\n",
    "        model.train()\n",
    "        avg_loss = 0.0\n",
    "        for batch in dataloader:\n",
    "            inputs, labels = batch\n",
    "            inputs, labels = inputs.to(device, non_blocking=True), labels.to(device, non_blocking=True)\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            scheduler.step()\n",
    "            avg_loss += loss.item()\n",
    "        if epoch % 10 == 0:\n",
    "            print(f\"Epoch {epoch+1}, loss: {avg_loss / len(dataloader)}\")\n",
    "\n",
    "def test_model(model, dataloader):\n",
    "    model.eval()\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    device = next(model.parameters()).device\n",
    "    with torch.no_grad():\n",
    "        for batch in tqdm(dataloader, desc=\"Testing\"):\n",
    "            inputs, labels = batch\n",
    "            inputs, labels = inputs.to(device, non_blocking=True), labels.to(device, non_blocking=True)\n",
    "            outputs = model(inputs)\n",
    "            _, pred = torch.max(outputs, 1)\n",
    "            total += labels.size(0)\n",
    "            correct += (pred == labels).sum().item()\n",
    "    return correct / total"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (a) Use Average Pooling Feature"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Binary Classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pretrained Word2Vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:   0%|          | 0/30 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:   3%|▎         | 1/30 [00:13<06:19, 13.10s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, loss: 0.3314122884068638\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  37%|███▋      | 11/30 [02:11<03:41, 11.64s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 11, loss: 0.22371997023001314\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  70%|███████   | 21/30 [04:07<01:44, 11.60s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 21, loss: 0.2046239526476711\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 30/30 [05:54<00:00, 11.80s/it]\n"
     ]
    }
   ],
   "source": [
    "num_epochs = 30\n",
    "batch_size = 500\n",
    "\n",
    "BinaryPretrainedAvgPoolingDataset = ReviewsDataset(reviews_train[labels_train != 3], labels_train[labels_train != 3], wv)\n",
    "BinaryPretrainedAvgPoolingDataloader = DataLoader(BinaryPretrainedAvgPoolingDataset, batch_size=batch_size, shuffle=True, num_workers=24)\n",
    "\n",
    "input_dim = BinaryPretrainedAvgPoolingDataset[0][0].shape[1]\n",
    "\n",
    "binary_mlp_1 = nn.Sequential(\n",
    "    nn.Flatten(),\n",
    "    nn.Linear(input_dim, 50),\n",
    "    nn.ReLU(),\n",
    "    nn.Linear(50, 10),\n",
    "    nn.ReLU(),\n",
    "    nn.Linear(10, 2),\n",
    ")\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "binary_mlp_1 = binary_mlp_1.to(device)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "criterion = criterion.to(device)\n",
    "optimizer = torch.optim.AdamW(binary_mlp_1.parameters(), lr=0.01)\n",
    "lr_scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=num_epochs*len(BinaryPretrainedAvgPoolingDataloader) // batch_size, eta_min=1e-6)\n",
    "\n",
    "train_model(binary_mlp_1, BinaryPretrainedAvgPoolingDataloader, criterion, optimizer, lr_scheduler, num_epochs=num_epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Testing:   0%|          | 0/80 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Testing: 100%|██████████| 80/80 [00:08<00:00,  9.25it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classification Accuracy: 0.90195\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "BinaryPretrainedAvgPoolingDataloader_test = DataLoader(ReviewsDataset(reviews_test[labels_test != 3], labels_test[labels_test != 3], wv), \n",
    "                                                 batch_size=500, shuffle=False, num_workers=24)\n",
    "print(\"Classification Accuracy:\", test_model(binary_mlp_1, BinaryPretrainedAvgPoolingDataloader_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Trained Word2Vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:   0%|          | 0/30 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:   3%|▎         | 1/30 [00:12<06:09, 12.76s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, loss: 0.225943755870685\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  37%|███▋      | 11/30 [02:13<03:55, 12.38s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 11, loss: 0.1656110317679122\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  70%|███████   | 21/30 [04:11<01:43, 11.53s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 21, loss: 0.1485034336335957\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 30/30 [05:58<00:00, 11.95s/it]\n"
     ]
    }
   ],
   "source": [
    "num_epochs = 30\n",
    "batch_size = 500\n",
    "\n",
    "BinaryTrainedAvgPoolingDataset = ReviewsDataset(reviews_train[labels_train != 3], labels_train[labels_train != 3], model.wv)\n",
    "BinaryTrainedAvgPoolingDataloader = DataLoader(BinaryTrainedAvgPoolingDataset, batch_size=batch_size, shuffle=True, num_workers=24)\n",
    "\n",
    "input_dim = BinaryTrainedAvgPoolingDataset[0][0].shape[1]\n",
    "\n",
    "binary_mlp_2 = nn.Sequential(\n",
    "    nn.Flatten(),\n",
    "    nn.Linear(input_dim, 50),\n",
    "    nn.ReLU(),\n",
    "    nn.Linear(50, 10),\n",
    "    nn.ReLU(),\n",
    "    nn.Linear(10, 2),\n",
    ")\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "binary_mlp_2 = binary_mlp_2.to(device)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "criterion = criterion.to(device)\n",
    "optimizer = torch.optim.AdamW(binary_mlp_2.parameters(), lr=0.01)\n",
    "lr_scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=num_epochs*len(BinaryTrainedAvgPoolingDataloader) // batch_size, eta_min=1e-6)\n",
    "\n",
    "train_model(binary_mlp_2, BinaryTrainedAvgPoolingDataloader, criterion, optimizer, lr_scheduler, num_epochs=num_epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Testing:   0%|          | 0/80 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Testing: 100%|██████████| 80/80 [00:09<00:00,  8.49it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classification Accuracy: 0.92155\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "BinaryTrainedAvgPoolingDataloader_test = DataLoader(ReviewsDataset(reviews_test[labels_test != 3], labels_test[labels_test != 3], model.wv), \n",
    "                                              batch_size=500, shuffle=False, num_workers=24)\n",
    "print(\"Classification Accuracy:\", test_model(binary_mlp_2, BinaryTrainedAvgPoolingDataloader_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Ternary Classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pretrained Word2Vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 216,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:   0%|          | 0/30 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:   3%|▎         | 1/30 [00:13<06:32, 13.55s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, loss: 0.8390966232120991\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  37%|███▋      | 11/30 [02:38<04:37, 14.61s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 11, loss: 0.581205048263073\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  70%|███████   | 21/30 [05:02<02:09, 14.39s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 21, loss: 0.5530290900915861\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 30/30 [07:09<00:00, 14.33s/it]\n"
     ]
    }
   ],
   "source": [
    "num_epochs = 30\n",
    "batch_size = 500\n",
    "\n",
    "TernaryPretrainedAvgPoolingDataset = ReviewsDataset(reviews_train, labels_train, wv)\n",
    "TernaryPretrainedAvgPoolingDataloader = DataLoader(TernaryPretrainedAvgPoolingDataset, batch_size=batch_size, shuffle=True, num_workers=24)\n",
    "\n",
    "input_dim = TernaryPretrainedAvgPoolingDataset[0][0].shape[1]\n",
    "\n",
    "ternary_mlp_1 = nn.Sequential(\n",
    "    nn.Flatten(),\n",
    "    nn.Linear(input_dim, 50),\n",
    "    nn.ReLU(),\n",
    "    nn.Linear(50, 10),\n",
    "    nn.ReLU(),\n",
    "    nn.Linear(10, 3),\n",
    ")\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "ternary_mlp_1 = ternary_mlp_1.to(device)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "criterion = criterion.to(device)\n",
    "optimizer = torch.optim.AdamW(ternary_mlp_1.parameters(), lr=0.001)\n",
    "lr_scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=num_epochs*len(TernaryPretrainedAvgPoolingDataloader) // batch_size, eta_min=1e-8)\n",
    "\n",
    "train_model(ternary_mlp_1, TernaryPretrainedAvgPoolingDataloader, criterion, optimizer, lr_scheduler, num_epochs=num_epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 217,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Testing: 100%|██████████| 100/100 [00:11<00:00,  8.64it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classification Accuracy: 0.7614\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "TernaryPretrainedAvgPoolingDataloader_test = DataLoader(ReviewsDataset(reviews_test, labels_test, wv), \n",
    "                                                 batch_size=500, shuffle=False, num_workers=24)\n",
    "print(\"Classification Accuracy:\", test_model(ternary_mlp_1, TernaryPretrainedAvgPoolingDataloader_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Trained Word2Vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 218,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:   3%|▎         | 1/30 [00:15<07:17, 15.09s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, loss: 0.6173438151180745\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  37%|███▋      | 11/30 [02:43<04:43, 14.90s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 11, loss: 0.4713097733259201\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  70%|███████   | 21/30 [05:12<02:13, 14.87s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 21, loss: 0.4543842290341854\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 30/30 [07:19<00:00, 14.64s/it]\n"
     ]
    }
   ],
   "source": [
    "num_epochs = 30\n",
    "batch_size = 500\n",
    "\n",
    "TernaryTrainedAvgPoolingDataset = ReviewsDataset(reviews_train, labels_train, model.wv)\n",
    "TernaryTrainedAvgPoolingDataloader = DataLoader(TernaryTrainedAvgPoolingDataset, batch_size=batch_size, shuffle=True, num_workers=24)\n",
    "\n",
    "input_dim = TernaryTrainedAvgPoolingDataset[0][0].shape[1]\n",
    "\n",
    "ternary_mlp_2 = nn.Sequential(\n",
    "    nn.Flatten(),\n",
    "    nn.Linear(input_dim, 50),\n",
    "    nn.ReLU(),\n",
    "    nn.Linear(50, 10),\n",
    "    nn.ReLU(),\n",
    "    nn.Linear(10, 3),\n",
    ")\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "ternary_mlp_2 = ternary_mlp_2.to(device)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "criterion = criterion.to(device)\n",
    "optimizer = torch.optim.AdamW(ternary_mlp_2.parameters(), lr=0.001)\n",
    "lr_scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=num_epochs*len(TernaryTrainedAvgPoolingDataloader) // batch_size, eta_min=1e-8)\n",
    "\n",
    "train_model(ternary_mlp_2, TernaryTrainedAvgPoolingDataloader, criterion, optimizer, lr_scheduler, num_epochs=num_epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 219,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Testing: 100%|██████████| 100/100 [00:09<00:00, 10.14it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classification Accuracy: 0.79716\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "TernaryTrainedAvgPoolingDataloader_test = DataLoader(ReviewsDataset(reviews_test, labels_test, model.wv),\n",
    "                                                batch_size=500, shuffle=False, num_workers=24)\n",
    "print(\"Classification Accuracy:\", test_model(ternary_mlp_2, TernaryTrainedAvgPoolingDataloader_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (b) Use Concatenated Features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Binary Classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pretrained Word2Vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 228,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:   0%|          | 0/30 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:   3%|▎         | 1/30 [00:13<06:28, 13.40s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, loss: 0.3983966983854771\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  37%|███▋      | 11/30 [02:20<04:01, 12.73s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 11, loss: 0.23861584621481596\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  70%|███████   | 21/30 [04:27<01:53, 12.60s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 21, loss: 0.1779147365130484\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 30/30 [06:22<00:00, 12.76s/it]\n"
     ]
    }
   ],
   "source": [
    "num_epochs = 30\n",
    "batch_size = 500\n",
    "\n",
    "BinaryPretrainedConcatDataset = ReviewsDataset(reviews_train[labels_train != 3], labels_train[labels_train != 3], wv, max_seq_len=10, average_pooling=False)\n",
    "BinaryPretrainedConcatDataloader = DataLoader(BinaryPretrainedConcatDataset, batch_size=batch_size, shuffle=True, num_workers=24)\n",
    "\n",
    "input_dim = BinaryPretrainedConcatDataset[0][0].shape[1] * BinaryPretrainedConcatDataset[0][0].shape[0]\n",
    "\n",
    "binary_mlp_3 = nn.Sequential(\n",
    "    nn.Flatten(),\n",
    "    nn.Linear(input_dim, 50),\n",
    "    nn.ReLU(),\n",
    "    nn.Linear(50, 10),\n",
    "    nn.ReLU(),\n",
    "    nn.Linear(10, 2),\n",
    ")\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "binary_mlp_3 = binary_mlp_3.to(device)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "criterion = criterion.to(device)\n",
    "optimizer = torch.optim.AdamW(binary_mlp_3.parameters(), lr=0.001)\n",
    "lr_scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=num_epochs*len(BinaryPretrainedConcatDataloader) // batch_size, eta_min=1e-6)\n",
    "\n",
    "train_model(binary_mlp_3, BinaryPretrainedConcatDataloader, criterion, optimizer, lr_scheduler, num_epochs=num_epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 229,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Testing:   0%|          | 0/80 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Testing: 100%|██████████| 80/80 [00:10<00:00,  7.62it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classification Accuracy: 0.85345\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "BinaryPretrainedConcatDataloader_test = DataLoader(ReviewsDataset(reviews_test[labels_test != 3], labels_test[labels_test != 3], wv, max_seq_len=10, average_pooling=False),\n",
    "                                                    batch_size=500, shuffle=False, num_workers=24)\n",
    "print(\"Classification Accuracy:\", test_model(binary_mlp_3, BinaryPretrainedConcatDataloader_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Trained Word2Vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 230,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:   3%|▎         | 1/30 [00:12<06:00, 12.44s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, loss: 0.29990119780413804\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  37%|███▋      | 11/30 [02:21<04:07, 13.01s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 11, loss: 0.08220909271622076\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  70%|███████   | 21/30 [04:28<01:54, 12.67s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 21, loss: 0.024188642197987064\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 30/30 [06:24<00:00, 12.81s/it]\n"
     ]
    }
   ],
   "source": [
    "num_epochs = 30\n",
    "batch_size = 500\n",
    "\n",
    "BinaryTrainedConcatDataset = ReviewsDataset(reviews_train[labels_train != 3], labels_train[labels_train != 3], model.wv, max_seq_len=10, average_pooling=False)\n",
    "BinaryTrainedConcatDataloader = DataLoader(BinaryTrainedConcatDataset, batch_size=batch_size, shuffle=True, num_workers=24)\n",
    "\n",
    "input_dim = BinaryTrainedConcatDataset[0][0].shape[1] * BinaryTrainedConcatDataset[0][0].shape[0]\n",
    "\n",
    "binary_mlp_4 = nn.Sequential(\n",
    "    nn.Flatten(),\n",
    "    nn.Linear(input_dim, 50),\n",
    "    nn.ReLU(),\n",
    "    nn.Linear(50, 10),\n",
    "    nn.ReLU(),\n",
    "    nn.Linear(10, 2),\n",
    ")\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "binary_mlp_4 = binary_mlp_4.to(device)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "criterion = criterion.to(device)\n",
    "optimizer = torch.optim.AdamW(binary_mlp_4.parameters(), lr=0.001)\n",
    "lr_scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=num_epochs*len(BinaryTrainedConcatDataloader) // batch_size, eta_min=1e-6)\n",
    "\n",
    "train_model(binary_mlp_4, BinaryTrainedConcatDataloader, criterion, optimizer, lr_scheduler, num_epochs=num_epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 231,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Testing: 100%|██████████| 80/80 [00:09<00:00,  8.34it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classification Accuracy: 0.8655\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "BinaryTrainedConcatDataloader_test = DataLoader(ReviewsDataset(reviews_test[labels_test != 3], labels_test[labels_test != 3], model.wv, max_seq_len=10, average_pooling=False),\n",
    "                                                    batch_size=500, shuffle=False, num_workers=24)\n",
    "print(\"Classification Accuracy:\", test_model(binary_mlp_4, BinaryTrainedConcatDataloader_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Ternary Classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pretrained Word2Vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 232,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:   3%|▎         | 1/30 [00:13<06:37, 13.70s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, loss: 0.7570322492718696\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  37%|███▋      | 11/30 [02:31<04:18, 13.61s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 11, loss: 0.447182969301939\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  70%|███████   | 21/30 [04:48<02:02, 13.62s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 21, loss: 0.31888479202985764\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 30/30 [06:52<00:00, 13.77s/it]\n"
     ]
    }
   ],
   "source": [
    "num_epochs = 30\n",
    "batch_size = 500\n",
    "\n",
    "TernaryPretrainedConcatDataset = ReviewsDataset(reviews_train, labels_train, wv, max_seq_len=10, average_pooling=False)\n",
    "TernaryPretrainedConcatDataloader = DataLoader(TernaryPretrainedConcatDataset, batch_size=batch_size, shuffle=True, num_workers=24)\n",
    "\n",
    "input_dim = TernaryPretrainedConcatDataset[0][0].shape[1] * TernaryPretrainedConcatDataset[0][0].shape[0]\n",
    "\n",
    "ternary_mlp_3 = nn.Sequential(\n",
    "    nn.Flatten(),\n",
    "    nn.Linear(input_dim, 50),\n",
    "    nn.ReLU(),\n",
    "    nn.Linear(50, 10),\n",
    "    nn.ReLU(),\n",
    "    nn.Linear(10, 3),\n",
    ")\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "ternary_mlp_3 = ternary_mlp_3.to(device)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "criterion = criterion.to(device)\n",
    "optimizer = torch.optim.AdamW(ternary_mlp_3.parameters(), lr=0.001)\n",
    "lr_scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=num_epochs*len(TernaryPretrainedConcatDataloader) // batch_size, eta_min=1e-8)\n",
    "\n",
    "train_model(ternary_mlp_3, TernaryPretrainedConcatDataloader, criterion, optimizer, lr_scheduler, num_epochs=num_epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 233,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Testing: 100%|██████████| 100/100 [00:09<00:00, 10.06it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classification Accuracy: 0.70042\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "TernaryPretrainedConcatDataloader_test = DataLoader(ReviewsDataset(reviews_test, labels_test, wv, max_seq_len=10, average_pooling=False),\n",
    "                                                    batch_size=500, shuffle=False, num_workers=24)\n",
    "print(\"Classification Accuracy:\", test_model(ternary_mlp_3, TernaryPretrainedConcatDataloader_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Trained Word2Vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 234,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:   3%|▎         | 1/30 [00:14<07:00, 14.50s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, loss: 0.6140345711261034\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  37%|███▋      | 11/30 [02:31<04:24, 13.93s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 11, loss: 0.3593694458156824\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  70%|███████   | 21/30 [04:49<02:05, 13.93s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 21, loss: 0.23860658537596463\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 30/30 [06:53<00:00, 13.80s/it]\n"
     ]
    }
   ],
   "source": [
    "num_epochs = 30\n",
    "batch_size = 500\n",
    "\n",
    "TernaryTrainedConcatDataset = ReviewsDataset(reviews_train, labels_train, model.wv, max_seq_len=10, average_pooling=False)\n",
    "TernaryTrainedConcatDataloader = DataLoader(TernaryTrainedConcatDataset, batch_size=batch_size, shuffle=True, num_workers=24)\n",
    "\n",
    "input_dim = TernaryTrainedConcatDataset[0][0].shape[1] * TernaryTrainedConcatDataset[0][0].shape[0]\n",
    "\n",
    "ternary_mlp_4 = nn.Sequential(\n",
    "    nn.Flatten(),\n",
    "    nn.Linear(input_dim, 50),\n",
    "    nn.ReLU(),\n",
    "    nn.Linear(50, 10),\n",
    "    nn.ReLU(),\n",
    "    nn.Linear(10, 3),\n",
    ")\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "ternary_mlp_4 = ternary_mlp_4.to(device)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "criterion = criterion.to(device)\n",
    "optimizer = torch.optim.AdamW(ternary_mlp_4.parameters(), lr=0.001)\n",
    "lr_scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=num_epochs*len(TernaryTrainedConcatDataloader) // batch_size, eta_min=1e-8)\n",
    "\n",
    "train_model(ternary_mlp_4, TernaryTrainedConcatDataloader, criterion, optimizer, lr_scheduler, num_epochs=num_epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 235,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Testing: 100%|██████████| 100/100 [00:11<00:00,  8.82it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classification Accuracy: 0.7196\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "TernaryTrainedConcatDataloader_test = DataLoader(ReviewsDataset(reviews_test, labels_test, model.wv, max_seq_len=10, average_pooling=False),\n",
    "                                                    batch_size=500, shuffle=False, num_workers=24)\n",
    "print(\"Classification Accuracy:\", test_model(ternary_mlp_4, TernaryTrainedConcatDataloader_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By using FFN models, word2vec features can get better results compared to using them on simple models, such as Perceptron and SVM. When using the same average pooling strategy, word2vec features on FFN models can get very close preformances compared to TF-IDF features on simple models. When using the first 10 words concatenation strategy, the performances are worse, which may be because the features don't capture the information of the whole review. Besides, our own trained word2vec gets better results than pretrained word2vec as in simple models."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Convolution Nerual Networks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Prepare CNN Model Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 236,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CNN(nn.Module):\n",
    "    def __init__(self, input_dim, output_dim, activation=nn.GELU):\n",
    "        super().__init__()\n",
    "        self.cnn = nn.Sequential(\n",
    "            nn.Conv1d(input_dim, 50, kernel_size=3, padding=1),\n",
    "            activation(),\n",
    "            nn.Conv1d(50, 10, kernel_size=3, padding=1),\n",
    "            activation(),\n",
    "            nn.AdaptiveMaxPool1d(1),\n",
    "        )\n",
    "        self.head = nn.Linear(10, output_dim)\n",
    "    def forward(self, x):\n",
    "        x = x.permute(0, 2, 1)\n",
    "        x = self.cnn(x)\n",
    "        x = x.squeeze(-1)\n",
    "        return self.head(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Binary Classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pretrained Word2Vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 237,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:   3%|▎         | 1/30 [00:22<10:55, 22.60s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, loss: 0.3904079989064485\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  37%|███▋      | 11/30 [03:36<06:09, 19.44s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 11, loss: 0.1769079332705587\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  70%|███████   | 21/30 [06:48<02:50, 18.98s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 21, loss: 0.14551416088361294\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 30/30 [09:43<00:00, 19.45s/it]\n"
     ]
    }
   ],
   "source": [
    "num_epochs = 30\n",
    "batch_size = 500\n",
    "\n",
    "BinaryPretrainedCNNDataset = ReviewsDataset(reviews_train[labels_train != 3], labels_train[labels_train != 3], wv, max_seq_len=50, average_pooling=False)\n",
    "BinaryPretrainedCNNDataloader = DataLoader(BinaryPretrainedCNNDataset, batch_size=batch_size, shuffle=True, num_workers=24)\n",
    "\n",
    "input_dim = BinaryPretrainedCNNDataset[0][0].shape[1]\n",
    "\n",
    "binary_cnn_1 = CNN(input_dim, 2)\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "binary_cnn_1 = binary_cnn_1.to(device)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "criterion = criterion.to(device)\n",
    "optimizer = torch.optim.AdamW(binary_cnn_1.parameters(), lr=0.001)\n",
    "lr_scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=num_epochs*len(BinaryPretrainedCNNDataloader) // batch_size, eta_min=1e-6)\n",
    "\n",
    "train_model(binary_cnn_1, BinaryPretrainedCNNDataloader, criterion, optimizer, lr_scheduler, num_epochs=num_epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 238,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Testing: 100%|██████████| 80/80 [00:13<00:00,  6.04it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classification Accuracy: 0.919525\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "BinaryPretrainedCNNDataloader_test = DataLoader(ReviewsDataset(reviews_test[labels_test != 3], labels_test[labels_test != 3], wv, max_seq_len=50, average_pooling=False),\n",
    "                                                    batch_size=500, shuffle=False, num_workers=24)\n",
    "print(\"Classification Accuracy:\", test_model(binary_cnn_1, BinaryPretrainedCNNDataloader_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Trained Word2Vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 245,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:   0%|          | 0/20 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:   5%|▌         | 1/20 [00:20<06:20, 20.03s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, loss: 0.2381597325205803\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  55%|█████▌    | 11/20 [03:37<02:56, 19.66s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 11, loss: 0.10566438717069104\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 20/20 [06:37<00:00, 19.86s/it]\n"
     ]
    }
   ],
   "source": [
    "num_epochs = 20\n",
    "batch_size = 500\n",
    "\n",
    "BinaryTrainedCNNDataset = ReviewsDataset(reviews_train[labels_train != 3], labels_train[labels_train != 3], model.wv, max_seq_len=50, average_pooling=False)\n",
    "BinaryTrainedCNNDataloader = DataLoader(BinaryTrainedCNNDataset, batch_size=batch_size, shuffle=True, num_workers=24)\n",
    "\n",
    "input_dim = BinaryTrainedCNNDataset[0][0].shape[1]\n",
    "\n",
    "binary_cnn_2 = CNN(input_dim, 2)\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "binary_cnn_2 = binary_cnn_2.to(device)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "criterion = criterion.to(device)\n",
    "optimizer = torch.optim.AdamW(binary_cnn_2.parameters(), lr=0.001)\n",
    "lr_scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=num_epochs*len(BinaryTrainedCNNDataloader) // batch_size, eta_min=1e-6)\n",
    "\n",
    "train_model(binary_cnn_2, BinaryTrainedCNNDataloader, criterion, optimizer, lr_scheduler, num_epochs=num_epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 246,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Testing:   0%|          | 0/80 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Testing: 100%|██████████| 80/80 [00:12<00:00,  6.36it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classification Accuracy: 0.9204\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "BinaryTrainedCNNDataloader_test = DataLoader(ReviewsDataset(reviews_test[labels_test != 3], labels_test[labels_test != 3], model.wv, max_seq_len=50, average_pooling=False),\n",
    "                                                    batch_size=500, shuffle=False, num_workers=24)\n",
    "print(\"Classification Accuracy:\", test_model(binary_cnn_2, BinaryTrainedCNNDataloader_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Ternary Classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pretrained Word2Vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 280,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:   0%|          | 0/30 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:   3%|▎         | 1/30 [00:22<10:51, 22.46s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, loss: 0.5850946374237538\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  37%|███▋      | 11/30 [04:04<06:58, 22.03s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 11, loss: 0.3853776513040066\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  70%|███████   | 21/30 [07:43<03:17, 21.95s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 21, loss: 0.33636606313288214\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 30/30 [11:01<00:00, 22.06s/it]\n"
     ]
    }
   ],
   "source": [
    "num_epochs = 30\n",
    "batch_size = 500\n",
    "\n",
    "TernaryPretrainedCNNDataset = ReviewsDataset(reviews_train, labels_train, wv, max_seq_len=50, average_pooling=False)\n",
    "TernaryPretrainedCNNDataloader = DataLoader(TernaryPretrainedCNNDataset, batch_size=batch_size, shuffle=True, num_workers=24)\n",
    "\n",
    "input_dim = TernaryPretrainedCNNDataset[0][0].shape[1]\n",
    "\n",
    "ternary_cnn_1 = CNN(input_dim, 3)\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "ternary_cnn_1 = ternary_cnn_1.to(device)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "criterion = criterion.to(device)\n",
    "optimizer = torch.optim.AdamW(ternary_cnn_1.parameters(), lr=0.01)\n",
    "lr_scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=num_epochs*len(TernaryPretrainedCNNDataloader) // batch_size, eta_min=1e-6)\n",
    "\n",
    "train_model(ternary_cnn_1, TernaryPretrainedCNNDataloader, criterion, optimizer, lr_scheduler, num_epochs=num_epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 281,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Testing: 100%|██████████| 100/100 [00:13<00:00,  7.15it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classification Accuracy: 0.77694\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "TernaryPretrainedCNNDataloader_test = DataLoader(ReviewsDataset(reviews_test, labels_test, wv, max_seq_len=50, average_pooling=False),\n",
    "                                                    batch_size=500, shuffle=False, num_workers=24)\n",
    "print(\"Classification Accuracy:\", test_model(ternary_cnn_1, TernaryPretrainedCNNDataloader_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Trained Word2Vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 282,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:   3%|▎         | 1/30 [00:22<10:49, 22.39s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, loss: 0.5495232558250427\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  37%|███▋      | 11/30 [04:05<07:04, 22.33s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 11, loss: 0.41949798181653025\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  70%|███████   | 21/30 [07:45<03:18, 22.10s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 21, loss: 0.38223037622869016\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 30/30 [11:05<00:00, 22.19s/it]\n"
     ]
    }
   ],
   "source": [
    "num_epochs = 30\n",
    "batch_size = 500\n",
    "\n",
    "TernaryTrainedCNNDataset = ReviewsDataset(reviews_train, labels_train, model.wv, max_seq_len=50, average_pooling=False)\n",
    "TernaryTrainedCNNDataloader = DataLoader(TernaryTrainedCNNDataset, batch_size=batch_size, shuffle=True, num_workers=24)\n",
    "\n",
    "input_dim = TernaryTrainedCNNDataset[0][0].shape[1]\n",
    "\n",
    "ternary_cnn_2 = CNN(input_dim, 3)\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "ternary_cnn_2 = ternary_cnn_2.to(device)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "criterion = criterion.to(device)\n",
    "optimizer = torch.optim.AdamW(ternary_cnn_2.parameters(), lr=0.01)\n",
    "lr_scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=num_epochs*len(TernaryTrainedCNNDataloader) // batch_size, eta_min=1e-6)\n",
    "\n",
    "train_model(ternary_cnn_2, TernaryTrainedCNNDataloader, criterion, optimizer, lr_scheduler, num_epochs=num_epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 283,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Testing: 100%|██████████| 100/100 [00:13<00:00,  7.41it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classification Accuracy: 0.78948\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "TernaryTrainedCNNDataloader_test = DataLoader(ReviewsDataset(reviews_test, labels_test, model.wv, max_seq_len=50, average_pooling=False),\n",
    "                                                    batch_size=500, shuffle=False, num_workers=24)\n",
    "print(\"Classification Accuracy:\", test_model(ternary_cnn_2, TernaryTrainedCNNDataloader_test))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
