{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CSCI544 HW3\n",
    "Yuhang Xiao - 6913860906 - yxiao776@usc.edu"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Environment requirements:\n",
    "- Python 3.11.5\n",
    "- Numpy\n",
    "- Pandas\n",
    "- json"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Report\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Vocabulary Creation\n",
    "Utilize pandas to process the data.\n",
    "- Threshold of replacement: 3\n",
    "- Size of vocabulary: 16920\n",
    "- Total occurrence of '< unk >': 32537\n",
    "\n",
    "#### Model Learning\n",
    "- Number of transition parameters: 2070\n",
    "- Number of emission parameters: 761400\n",
    "\n",
    "### Greedy Decoding with HMM\n",
    "- Accuracy on dev set: 0.9298995203691336\n",
    "\n",
    "### Viterbi Decoding with HMM\n",
    "- Accuracy on dev set: 0.9437116750652662"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import json"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Vocabulary Creation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Threshold of replacement: 3\n",
      "Size of vocabulary: 16920\n",
      "Total occurrence of '< unk >': 32537\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>word</th>\n",
       "      <th>index</th>\n",
       "      <th>occurrence</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>&lt; unk &gt;</td>\n",
       "      <td>1</td>\n",
       "      <td>32537</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>,</td>\n",
       "      <td>2</td>\n",
       "      <td>46476</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>the</td>\n",
       "      <td>3</td>\n",
       "      <td>39533</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>.</td>\n",
       "      <td>4</td>\n",
       "      <td>37452</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>of</td>\n",
       "      <td>5</td>\n",
       "      <td>22104</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      word  index  occurrence\n",
       "0  < unk >      1       32537\n",
       "1        ,      2       46476\n",
       "2      the      3       39533\n",
       "3        .      4       37452\n",
       "4       of      5       22104"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = pd.read_csv('data/train', sep='\\t', names=['index', 'word', 'tag'])\n",
    "data['word'] = data['word'].fillna('')\n",
    "data['occurrence'] = data.groupby('word')['word'].transform('count')\n",
    "threshold = 3\n",
    "data['word'] = data.apply(lambda x: '< unk >' if x.occurrence < threshold else x.word, axis=1)\n",
    "vocab = data.word.value_counts().rename_axis('word').reset_index(name='occurrence')\n",
    "vocab = pd.concat([vocab[vocab.word == '< unk >'], vocab.drop(vocab[vocab.word == '< unk >'].index)]).reset_index(drop=True)\n",
    "vocab['index'] = vocab.index + 1\n",
    "vocab = vocab[['word', 'index', 'occurrence']]\n",
    "vocab.to_csv('vocab.txt', sep='\\t', index=False, header=False)\n",
    "print('Threshold of replacement:', threshold)\n",
    "print('Size of vocabulary:', vocab.shape[0])\n",
    "print('Total occurrence of \\'< unk >\\':', vocab.iloc[0]['occurrence'])\n",
    "vocab.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Index the Part-Of-Speech (POS) tags\n",
    "A special < bos > tag is added here for initial probability."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of POS tags: 46\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tag</th>\n",
       "      <th>count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>&lt; bos &gt;</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>NN</td>\n",
       "      <td>127534</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>IN</td>\n",
       "      <td>94758</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>NNP</td>\n",
       "      <td>87608</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>DT</td>\n",
       "      <td>78775</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       tag   count\n",
       "0  < bos >       0\n",
       "1       NN  127534\n",
       "2       IN   94758\n",
       "3      NNP   87608\n",
       "4       DT   78775"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tag = data.tag.value_counts().rename_axis('tag').reset_index(name='count')\n",
    "# create a new tag for the beginning of a sentence\n",
    "tag = pd.concat([pd.DataFrame({'tag': ['< bos >'], 'count': [0]}), tag]).reset_index(drop=True)\n",
    "print('Number of POS tags:', tag.shape[0])\n",
    "tag.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create index dict for vocab and tags"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "get the index dict for tags and vocabulary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "tag_dict = pd.Series(tag.index.values, index=tag.tag).to_dict()\n",
    "vocab_dict = pd.Series(vocab.index.values, index=vocab.word).to_dict()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Transition Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Transition matrix: 46 x 46\n"
     ]
    }
   ],
   "source": [
    "trans = np.zeros((tag.shape[0], tag.shape[0]))\n",
    "for row in data.itertuples():\n",
    "    if row.index == 1:\n",
    "        tag.at[0, 'count'] += 1\n",
    "        i = 0\n",
    "        j = tag_dict[row.tag]\n",
    "        trans[i, j] += 1\n",
    "    else:\n",
    "        i = tag_dict[data.at[row.Index - 1, 'tag']]\n",
    "        j = tag_dict[row.tag]\n",
    "        trans[i, j] += 1\n",
    "trans /= tag['count'].to_numpy().reshape(-1, 1)\n",
    "print('Transition matrix:', trans.shape[0], 'x', trans.shape[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Emission Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Emission matrix: 46 x 16920\n"
     ]
    }
   ],
   "source": [
    "emis = np.zeros((tag.shape[0], vocab.shape[0])) \n",
    "for row in data.itertuples():\n",
    "    i = tag_dict[row.tag]\n",
    "    j = vocab_dict[row.word]\n",
    "    emis[i, j] += 1\n",
    "emis /= tag['count'].to_numpy().reshape(-1, 1)\n",
    "print('Emission matrix:', emis.shape[0], 'x', emis.shape[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Transfer Matrix to Dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of transition parameters: 2070\n",
      "Number of emission parameters: 761400\n"
     ]
    }
   ],
   "source": [
    "transition = dict()\n",
    "for i in range(trans.shape[0]):\n",
    "    for j in range(1, trans.shape[1]):\n",
    "        transition['('+tag.at[i, 'tag']+','+tag.at[j, 'tag']+')'] = trans[i, j]\n",
    "emission = dict()\n",
    "for i in range(1, emis.shape[0]):\n",
    "    for j in range(emis.shape[1]):\n",
    "        emission['('+tag.at[i, 'tag']+','+vocab.at[j, 'word']+')'] = emis[i, j]\n",
    "print('Number of transition parameters:', len(transition))\n",
    "print('Number of emission parameters:', len(emission))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save the JSON"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('hmm.json', 'w') as f:\n",
    "    json.dump({'transition': transition, 'emission': emission}, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read Dev and Test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "dev_data = pd.read_csv('data/dev', sep='\\t', names=['index', 'word', 'tag'])\n",
    "dev_data['word'] = dev_data['word'].fillna('')\n",
    "test_data = pd.read_csv('data/test', sep='\\t', names=['index', 'word'])\n",
    "test_data['word'] = test_data['word'].fillna('')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Greedy Decoding with HMM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Greedy decoding function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def greedy_decode(data, tag, tag_dict, vocab_dict, trans, emis):\n",
    "    pred_tag = []\n",
    "    for line in data.itertuples():\n",
    "        if line.index == 1:\n",
    "            states = trans[0, 1:]\n",
    "        else:\n",
    "            states = trans[tag_dict[pred_tag[-1]], 1:]\n",
    "        words = emis[1:, vocab_dict[line.word] if line.word in vocab_dict else vocab_dict['< unk >']]\n",
    "        tag_idx = np.argmax(states * words)+1\n",
    "        pred_tag.append(tag.at[tag_idx, 'tag'])\n",
    "    return pred_tag      "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "dev data evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on dev set: 0.9298995203691336\n"
     ]
    }
   ],
   "source": [
    "dev_pred = greedy_decode(dev_data, tag, tag_dict, vocab_dict, trans, emis)\n",
    "acc = 0\n",
    "total = len(dev_pred)\n",
    "assert total == dev_data.shape[0], 'Number of predictions does not match number of words'\n",
    "for line in dev_data.itertuples():\n",
    "    if line.tag == dev_pred[line.Index]:\n",
    "        acc += 1\n",
    "print('Accuracy on dev set:', acc/total)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "store test data prediction results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_pred = greedy_decode(test_data, tag, tag_dict, vocab_dict, trans, emis)\n",
    "assert len(test_pred) == test_data.shape[0], 'Number of predictions does not match number of words'\n",
    "with open('greedy.out', 'w') as f:\n",
    "    for line in test_data.itertuples():\n",
    "        if line.Index == test_data.shape[0]:\n",
    "            f.write(str(line.index)+'\\t'+line.word+'\\t'+test_pred[line.Index])\n",
    "        else:\n",
    "            f.write(str(line.index)+'\\t'+line.word+'\\t'+test_pred[line.Index]+'\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Viterbi Decoding with HMM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Viterbi Decoding Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def viterbi_decode(data, tag, vocab_dict, trans, emis):\n",
    "    def forward(seq):\n",
    "        dp = np.zeros((len(seq), tag.shape[0]-1))\n",
    "        parent = np.zeros((len(seq), tag.shape[0]-1), dtype=int)\n",
    "        dp[0] = trans[0, 1:] * emis[1:, vocab_dict[seq[0]] if seq[0] in vocab_dict else vocab_dict['< unk >']]\n",
    "        for i in range(1, len(seq)):\n",
    "            arr = dp[[i-1]].T * trans[1:, 1:] * emis[1:, vocab_dict[seq[i]] if seq[i] in vocab_dict else vocab_dict['< unk >']]\n",
    "            parent[i] = np.argmax(arr, axis=0)\n",
    "            dp[i] = arr[parent[i], np.arange(dp.shape[1])]\n",
    "        return dp, parent\n",
    "    def backward(dp, parent):\n",
    "        pred_tag = []\n",
    "        last_state = np.argmax(dp[-1])\n",
    "        pred_tag.append(tag.at[last_state+1, 'tag'])\n",
    "        for i in range(dp.shape[0]-2,-1,-1):\n",
    "            last_state = parent[i+1, last_state]\n",
    "            pred_tag.append(tag.at[last_state+1, 'tag'])\n",
    "        return pred_tag[::-1]\n",
    "    seq = []\n",
    "    pred = []\n",
    "    for line in data.itertuples():\n",
    "        if line.index == 1:\n",
    "            if len(seq) > 0:\n",
    "                dp, parent = forward(seq)\n",
    "                pred_tag = backward(dp, parent)\n",
    "                pred.extend(pred_tag)\n",
    "            seq = [line.word]\n",
    "        else:\n",
    "            seq.append(line.word)\n",
    "    if len(seq) > 0: pred.extend(backward(*forward(seq)))\n",
    "    return pred "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "dev data evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on dev set: 0.9437116750652662\n"
     ]
    }
   ],
   "source": [
    "dev_pred = viterbi_decode(dev_data, tag, vocab_dict, trans, emis)\n",
    "acc = 0\n",
    "total = len(dev_pred)\n",
    "assert total == dev_data.shape[0], 'Number of predictions does not match number of words'\n",
    "for line in dev_data.itertuples():\n",
    "    if line.tag == dev_pred[line.Index]:\n",
    "        acc += 1\n",
    "print('Accuracy on dev set:', acc/total)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "store test data prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_pred = viterbi_decode(test_data, tag, vocab_dict, trans, emis)\n",
    "assert len(test_pred) == test_data.shape[0], 'Number of predictions does not match number of words'\n",
    "with open('viterbi.out', 'w') as f:\n",
    "    for line in test_data.itertuples():\n",
    "        if line.Index == test_data.shape[0]:\n",
    "            f.write(str(line.index)+'\\t'+line.word+'\\t'+test_pred[line.Index])\n",
    "        else:\n",
    "            f.write(str(line.index)+'\\t'+line.word+'\\t'+test_pred[line.Index]+'\\n')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
